% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SS.R
\name{RSS}
\alias{RSS}
\title{Residual Sum of Square}
\usage{
RSS(X, y, betahat = NULL)
}
\arguments{
\item{X}{Matrix.
The data matrix
\eqn{\mathbf{X}}
(also known as
design matrix,
model matrix or
regressor matrix)
is an \eqn{n \times k} matrix
of \eqn{n} observations
of \eqn{k} regressors,
which includes a regressor
whose value is 1 for each observation.}

\item{y}{Vector or \code{n} by \code{1} matrix.
The vector
\eqn{\mathbf{y}}
is an \eqn{n \times 1} vector
of observations on the regressand variable.}

\item{betahat}{Vector or \code{k} by \code{1} matrix.
The vector
\eqn{\boldsymbol{\hat{\beta}}}
is a \eqn{k \times 1} vector of estimates
of \eqn{k} unknown regression coefficients.}
}
\value{
Returns residual sum of squares (\eqn{RSS}).
}
\description{
Calculates the residual sum of squares (RSS) using
\deqn{
    RSS
    =
    \sum_{i = 1}^{n}
    \left(
      Y_i
      -
      \hat{Y}_i
    \right)^2 \\
    =
    \sum_{i = 1}^{n}
    \left(
      Y_i
      -
      \left[
        \hat{\beta}_{1}
        +
        \hat{\beta}_{2}
        X_{2i}
        +
        \hat{\beta}_{3}
        X_{3i}
        +
        \dots
        +
        \hat{\beta}_{k}
        X_{ki}
      \right]
    \right)^2 \\
    =
    \sum_{i = 1}^{n}
    \left(
      Y_i
      -
      \hat{\beta}_{1}
      -
      \hat{\beta}_{2}
      X_{2i}
      -
      \hat{\beta}_{3}
      X_{3i}
      -
      \dots
      -
      \hat{\beta}_{k}
      X_{ki}
    \right)^2 .
  }
In matrix form
\deqn{
    RSS
    =
    \sum_{i = 1}^{n}
    \left(
      \mathbf{y}
      -
      \mathbf{\hat{y}}
    \right)^{2} \\
    =
    \sum_{i = 1}^{n}
    \left(
      \mathbf{y}
      -
      \mathbf{X}
      \boldsymbol{\hat{\beta}}
    \right)^{2} \\
    =
    \left(
      \mathbf{y}
      -
      \mathbf{X}
      \boldsymbol{\hat{\beta}}
    \right)^{\prime}
    \left(
      \mathbf{y}
      -
      \mathbf{X}
      \boldsymbol{\hat{\beta}}
    \right) .
  }
Or simply
\deqn{
    RSS
    =
    \sum_{i = 1}^{n}
    \boldsymbol{\hat{\varepsilon}}_{i}^{2}
    =
    \boldsymbol{\hat{\varepsilon}}^{\prime}
    \boldsymbol{\hat{\varepsilon}}
  }
where
\eqn{\boldsymbol{\hat{\varepsilon}}}
is an \eqn{n \times 1}
vector of residuals,
that is,
the difference between
the observed and predicted value of \eqn{\mathbf{y}}
(\eqn{\boldsymbol{\hat{\varepsilon}} = \mathbf{y}} - \eqn{\mathbf{\hat{y}}}).
Equivalent computational matrix formula
\deqn{
    RSS
    =
    \mathbf{y}^{\prime}
    \mathbf{y}
    -
    2
    \boldsymbol{\hat{\beta}}
    \mathbf{X}^{\prime}
    \mathbf{y}
    +
    \boldsymbol{\hat{\beta}}^{\prime}
    \mathbf{X}^{\prime}
    \mathbf{X}
    \boldsymbol{\hat{\beta}}.
  }
Note that
\deqn{
    TSS
    =
    ESS
    +
    RSS.
  }
}
\details{
If \code{betahat = NULL},
\code{betahat} computed
using \code{\link[=betahat_inv]{betahat_inv()}}.
}
\references{
\href{https://en.wikipedia.org/wiki/Residual_sum_of_squares}{Wikipedia: Residual Sum of Squares}

\href{https://en.wikipedia.org/wiki/Explained_sum_of_squares}{Wikipedia: Explained Sum of Squares}

\href{https://en.wikipedia.org/wiki/Total_sum_of_squares}{Wikipedia: Total Sum of Squares}

\href{https://en.wikipedia.org/wiki/Coefficient_of_determination}{Wikipedia: Coefficient of Determination}
}
\seealso{
Other sum of squares functions: 
\code{\link{.ESS_raw}()},
\code{\link{.ESS}()},
\code{\link{.RSS_raw}()},
\code{\link{.RSS}()},
\code{\link{ESS}()},
\code{\link{TSS}()}
}
\author{
Ivan Jacob Agaloos Pesigan
}
\concept{sum of squares functions}
