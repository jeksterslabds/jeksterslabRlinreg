% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/betahat.R
\name{.betahatqr}
\alias{.betahatqr}
\title{Regression Coefficients
\eqn{\boldsymbol{\hat{\beta}}} - QR Decomposition}
\usage{
.betahatqr(X, y)
}
\arguments{
\item{X}{\code{n} by \code{k} numeric matrix.
The data matrix \eqn{\mathbf{X}}
(also known as design matrix, model matrix or regressor matrix)
is an \eqn{n \times k} matrix of \eqn{n} observations of \eqn{k} regressors,
which includes a regressor whose value is 1 for each observation on the first column.}

\item{y}{Numeric vector of length \code{n} or \code{n} by \code{1} matrix.
The vector \eqn{\mathbf{y}} is an \eqn{n \times 1} vector of observations
on the regressand variable.}
}
\value{
Returns \eqn{\boldsymbol{\hat{\beta}}}, that is,
a \eqn{k \times 1} vector of estimates
of \eqn{k} unknown regression coefficients
estimated using ordinary least squares.
}
\description{
Estimates coefficients of a linear regression model
using QR Decomposition.
The data matrix \eqn{\mathbf{X}} is decomposed into
\deqn{
    \mathbf{X} = \mathbf{Q} \mathbf{R} .
  }
Estimates are found by solving \eqn{\boldsymbol{\hat{\beta}}} in
\deqn{
    \mathbf{R} \boldsymbol{\hat{\beta}} = \mathbf{Q}^{T} \mathbf{y}.
  }
}
\references{
\href{https://en.wikipedia.org/wiki/Linear_regression}{Wikipedia: Linear Regression}

\href{https://en.wikipedia.org/wiki/Ordinary_least_squares}{Wikipedia: Ordinary Least Squares}

\href{https://en.wikipedia.org/wiki/QR_decomposition}{Wikipedia: QR Decomposition}

\href{https://en.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares#Orthogonal_decomposition_methods}{Wikipedia: Orthogonal decomposition methods}

\href{https://en.wikipedia.org/wiki/Design_matrix}{Wikipedia: Design Matrix}
}
\seealso{
Other beta-hat functions: 
\code{\link{.betahatnorm}()},
\code{\link{.betahatsvd}()},
\code{\link{betahat}()}
}
\author{
Ivan Jacob Agaloos Pesigan
}
\concept{beta-hat functions}
\keyword{beta-hat-ols}
