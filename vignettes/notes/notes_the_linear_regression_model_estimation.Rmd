---
title: "Notes: The Linear Regression Model - Estimation"
author: "Ivan Jacob Agaloos Pesigan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Notes: The Linear Regression Model - Estimation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Because we rarely observe the whole population,
given the values of the regressors,
we draw a random sample of $Y_i$ values and estimate
$\mathbb{E} \left( Y_i \mid \mathbf{X} \right)$.
Based on the estimate,
we try to infer the true value of
$\mathbb{E} \left( Y_i \mid \mathbf{X} \right)$.

Given sample data,
the estimated model is written as

\begin{equation}
\label{eq:regression_sample}
  \mathbf{y}
  =
  \mathbf{X}
  \boldsymbol{\hat{\beta}}
  +
  \boldsymbol{\hat{\varepsilon}} 
\end{equation}

\noindent where
$\boldsymbol{\hat{\beta}}$
is a $k \times 1$ colum vector of estimators
of $\boldsymbol{\beta}$
and
$\boldsymbol{\hat{\varepsilon}}$,
called the $n \times 1$ colum vector
of residuals,
is the sample counterpart of
$\boldsymbol{\varepsilon}$.

Based on a random sample of $\mathbf{y}$
and a fixed $\mathbf{X}$,
we estimate the parameters
$\boldsymbol{\beta}$
using the method of ordinary least squares (OLS).

\begin{equation}
  Y_i
  =
  \hat{\beta}_1
  +
  \hat{\beta}_2 X_{2i}
  + 
  \hat{\beta}_3 X_{3i}
  +
  \dots
  +
  \hat{\beta}_k X_{ki}
  +
  \hat{\varepsilon}_i
\end{equation}

\begin{equation}
  \hat{\varepsilon}_i
  =
  Y_i
  -
  \hat{\beta}_1
  -
  \hat{\beta}_2 X_{2i}
  - 
  \hat{\beta}_3 X_{3i}
  -
  \dots
  -
  \hat{\beta}_k X_{ki}
\end{equation}

\begin{equation}
  \boldsymbol{\hat{\varepsilon}}
  =
  \mathbf{y}
  -
  \mathbf{X}
  \boldsymbol{\hat{\beta}}
\end{equation}

\begin{equation}
  \sum_{i = 1}^{n}
  \hat{\varepsilon}^{2}_{i}
  =
  \sum_{i = 1}^{n}
  \left(
    Y_i
    -
    \hat{\beta}_1
    -
    \hat{\beta}_2 X_{2i}
    - 
    \hat{\beta}_3 X_{3i}
    -
    \dots
    -
    \hat{\beta}_k X_{ki}
  \right)^2
\end{equation}

\begin{equation}
\begin{split}
  \sum_{i = 1}^{n}
  \hat{\varepsilon}^{2}_{i}
  &=
  \boldsymbol{\hat{\varepsilon}}^{\prime}
  \boldsymbol{\hat{\varepsilon}} \\
  &=
  \left(
    \mathbf{y}
    -
    \mathbf{X}
    \boldsymbol{\hat{\beta}}
  \right)^{\prime}
  \left(
    \mathbf{y}
    -
    \mathbf{X}
    \boldsymbol{\hat{\beta}}
  \right) \\
  &=
  \mathbf{y}^{\prime}
  \mathbf{y}
  -
  2
  \boldsymbol{\hat{\beta}}
  \mathbf{X}^{\prime}
  \mathbf{y}
  +
  \boldsymbol{\hat{\beta}}^{\prime}
  \mathbf{X}^{\prime}
  \mathbf{X}
  \boldsymbol{\hat{\beta}}
\end{split}
\end{equation}
